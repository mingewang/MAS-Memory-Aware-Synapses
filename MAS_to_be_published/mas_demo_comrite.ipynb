{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "# autoreload in case we changed source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from MAS import *                                                               \n",
    "from MNIST_split import *                                                       \n",
    "import os                                                                       \n",
    "                                                                                \n",
    "# prepare dir                                                                   \n",
    "os.system(\"mkdir -p data/Pytorch_MNIST_dataset\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100                                                                  \n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}                                 \n",
    "train_loader = torch.utils.data.DataLoader(                                     \n",
    "    torchvision.datasets.MNIST('data', train=True, download=True,               \n",
    "                   transform=transforms.Compose([                               \n",
    "                       transforms.ToTensor(),                                   \n",
    "                       transforms.Normalize((0.1307,), (0.3081,))               \n",
    "                   ])),                                                         \n",
    "    batch_size=batch_size, shuffle=True, **kwargs)                              \n",
    "test_loader = torch.utils.data.DataLoader(                                      \n",
    "    torchvision.datasets.MNIST('../data', train=False, download=True,           \n",
    "                   transform=transforms.Compose([                               \n",
    "                       transforms.ToTensor(),                                   \n",
    "                       transforms.Normalize((0.1307,), (0.3081,))               \n",
    "                   ])),                                                         \n",
    "    batch_size=batch_size, shuffle=True, **kwargs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t10k-images-idx3-ubyte\ttrain-images-idx3-ubyte\r\n",
      "t10k-labels-idx1-ubyte\ttrain-labels-idx1-ubyte\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.pt  training.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwang/pytorch-cpu/lib/python3.7/site-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/home/mwang/pytorch-cpu/lib/python3.7/site-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/home/mwang/pytorch-cpu/lib/python3.7/site-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n",
      "/home/mwang/pytorch-cpu/lib/python3.7/site-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "# we split the full MNIST training                                              \n",
    "# data set into 5 subsets of consecutive digits. The 5 tasks                    \n",
    "# correspond to learning to distinguish between two consecutive digits from 0 to 10.\n",
    "for digits in [[1,2],[3,4],[5,6],[7,8],[9,0]]:                                  \n",
    "    dsets={}                                                                    \n",
    "    dsets['train']=    MNIST_Split('data', train=True, download=True,           \n",
    "                       transform=transforms.Compose([                           \n",
    "                           transforms.ToTensor(),                               \n",
    "                           transforms.Normalize((0.1307,), (0.3081,))           \n",
    "                       ]),digits=digits)                                        \n",
    "    dsets['val']=  MNIST_Split('data', train=False, download=True,              \n",
    "                      transform=transforms.Compose([                            \n",
    "                           transforms.ToTensor(),                               \n",
    "                           transforms.Normalize((0.1307,), (0.3081,))           \n",
    "                       ]),digits=digits)                                        \n",
    "    dlabel=''                                                                   \n",
    "    for i in digits:                                                            \n",
    "        dlabel=dlabel+str(i)                                                    \n",
    "    torch.save(dsets,'data/Pytorch_MNIST_dataset//split'+dlabel+'_dataset.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split12_dataset.pth.tar  split56_dataset.pth.tar  split90_dataset.pth.tar\r\n",
      "split34_dataset.pth.tar  split78_dataset.pth.tar\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/Pytorch_MNIST_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='General_utils/mnist_net.pth.tar' \n",
    "pretrained_model=torch.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNIST_Net(\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr is 0.01\n",
      "dictoinary length2\n",
      "=> no checkpoint found at 'exp_dir/SGD_MNIST_NET12/epoch.pth.tar'\n",
      "0\n",
      "Epoch 0/9\n",
      "----------\n",
      "lr is 0.01\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0008 Acc: 0.9554\n",
      "val Loss: 0.0002 Acc: 0.9917\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "lr is 0.01\n",
      "train Loss: 0.0002 Acc: 0.9910\n",
      "val Loss: 0.0001 Acc: 0.9922\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "lr is 0.01\n",
      "train Loss: 0.0001 Acc: 0.9927\n",
      "val Loss: 0.0001 Acc: 0.9935\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "lr is 0.01\n",
      "train Loss: 0.0001 Acc: 0.9941\n",
      "val Loss: 0.0001 Acc: 0.9931\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "lr is 0.01\n",
      "train Loss: 0.0001 Acc: 0.9951\n",
      "val Loss: 0.0001 Acc: 0.9935\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "lr is 0.01\n",
      "train Loss: 0.0001 Acc: 0.9958\n",
      "val Loss: 0.0001 Acc: 0.9935\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "lr is 0.01\n",
      "train Loss: 0.0001 Acc: 0.9969\n",
      "val Loss: 0.0001 Acc: 0.9945\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "lr is 0.01\n",
      "train Loss: 0.0001 Acc: 0.9972\n",
      "val Loss: 0.0001 Acc: 0.9935\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "lr is 0.01\n",
      "train Loss: 0.0001 Acc: 0.9976\n",
      "val Loss: 0.0001 Acc: 0.9945\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "lr is 0.01\n",
      "train Loss: 0.0000 Acc: 0.9976\n",
      "val Loss: 0.0001 Acc: 0.9949\n",
      "\n",
      "Training complete in 0m 32s\n",
      "Best val Acc: 0.994924\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "#FIRST TASK TRAINING [1,2]                                                      \n",
    "model_path='General_utils/mnist_net.pth.tar'                                    \n",
    "from Finetune_SGD import *                                                      \n",
    "digits = [1,2]                                                                  \n",
    "dlabel=''                                                                       \n",
    "for i in digits:                                                                \n",
    "    dlabel=dlabel+str(i)                                                        \n",
    "                                                                                \n",
    "dataset_path='data/Pytorch_MNIST_dataset//split'+dlabel+'_dataset.pth.tar'      \n",
    "                                                                                \n",
    "exp_dir='exp_dir/SGD_MNIST_NET'+dlabel                                          \n",
    "                                                                                \n",
    "num_epochs=10                                                                   \n",
    "       \n",
    "\n",
    "fine_tune_SGD(dataset_path=dataset_path, num_epochs=num_epochs, exp_dir=exp_dir,\n",
    "              model_path=model_path, lr=0.01, batch_size=200)  \n",
    "\n",
    "model_path=os.path.join(exp_dir,'best_model.pth.tar')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3156\r\n",
      "-rw-r--r-- 1 mwang mwang 1077934 Sep 28 12:46 best_model.pth.tar\r\n",
      "-rw-r--r-- 1 mwang mwang 2148961 Sep 28 12:46 epoch.pth.tar\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l exp_dir/SGD_MNIST_NET12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing param classifier.0.weight\n",
      "initializing param classifier.0.bias\n",
      "initializing param classifier.2.weight\n",
      "initializing param classifier.2.bias\n",
      "initializing param classifier.4.weight\n",
      "initializing param classifier.4.bias\n",
      "********************objective with L2 norm***************\n",
      "dictoinary length1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwang/pytorch-cpu/lib/python3.7/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n",
      "/home/mwang/pytorch-cpu/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch number  0\n",
      "batch number  1\n",
      "batch number  2\n",
      "batch number  3\n",
      "batch number  4\n",
      "batch number  5\n",
      "batch number  6\n",
      "batch number  7\n",
      "batch number  8\n",
      "batch number  9\n",
      "batch number  10\n",
      "batch number  11\n",
      "batch number  12\n",
      "batch number  13\n",
      "batch number  14\n",
      "batch number  15\n",
      "batch number  16\n",
      "batch number  17\n",
      "batch number  18\n",
      "batch number  19\n",
      "batch number  20\n",
      "batch number  21\n",
      "batch number  22\n",
      "batch number  23\n",
      "batch number  24\n",
      "batch number  25\n",
      "batch number  26\n",
      "batch number  27\n",
      "batch number  28\n",
      "batch number  29\n",
      "batch number  30\n",
      "batch number  31\n",
      "batch number  32\n",
      "batch number  33\n",
      "batch number  34\n",
      "batch number  35\n",
      "batch number  36\n",
      "batch number  37\n",
      "batch number  38\n",
      "batch number  39\n",
      "batch number  40\n",
      "batch number  41\n",
      "batch number  42\n",
      "batch number  43\n",
      "batch number  44\n",
      "batch number  45\n",
      "batch number  46\n",
      "batch number  47\n",
      "batch number  48\n",
      "batch number  49\n",
      "batch number  50\n",
      "batch number  51\n",
      "batch number  52\n",
      "batch number  53\n",
      "batch number  54\n",
      "batch number  55\n",
      "batch number  56\n",
      "batch number  57\n",
      "batch number  58\n",
      "batch number  59\n",
      "batch number  60\n",
      "batch number  61\n",
      "batch number  62\n",
      "batch number  63\n",
      "classifier.0.weight\n",
      "omega max is tensor(7.6210)\n",
      "omega min is tensor(0.0004)\n",
      "omega mean is tensor(0.2654)\n",
      "classifier.0.bias\n",
      "omega max is tensor(3.4149)\n",
      "omega min is tensor(0.0011)\n",
      "omega mean is tensor(0.5016)\n",
      "classifier.2.weight\n",
      "omega max is tensor(6.3572)\n",
      "omega min is tensor(0.)\n",
      "omega mean is tensor(0.2082)\n",
      "classifier.2.bias\n",
      "omega max is tensor(2.4019)\n",
      "omega min is tensor(0.)\n",
      "omega mean is tensor(0.4899)\n",
      "classifier.4.weight\n",
      "omega max is tensor(12.2244)\n",
      "omega min is tensor(0.)\n",
      "omega mean is tensor(2.7071)\n",
      "classifier.4.bias\n",
      "omega max is tensor(9.0094)\n",
      "omega min is tensor(9.0087)\n",
      "omega mean is tensor(9.0091)\n",
      "classifier.0.weight\n",
      "omega max is tensor(7.6210)\n",
      "omega min is tensor(0.0004)\n",
      "omega mean is tensor(0.2654)\n",
      "classifier.0.bias\n",
      "omega max is tensor(3.4149)\n",
      "omega min is tensor(0.0011)\n",
      "omega mean is tensor(0.5016)\n",
      "classifier.2.weight\n",
      "omega max is tensor(6.3572)\n",
      "omega min is tensor(0.)\n",
      "omega mean is tensor(0.2082)\n",
      "classifier.2.bias\n",
      "omega max is tensor(2.4019)\n",
      "omega min is tensor(0.)\n",
      "omega mean is tensor(0.4899)\n",
      "classifier.4.weight\n",
      "classifier.4.bias\n",
      "dictoinary length2\n",
      "=> no checkpoint found at 'exp_dir/SGD_MNIST_NET34/epoch.pth.tar'\n",
      "0\n",
      "Epoch 0/19\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0008 Acc: 0.9607\n",
      "val Loss: 0.0002 Acc: 0.9950\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9927\n",
      "val Loss: 0.0001 Acc: 0.9960\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9943\n",
      "val Loss: 0.0001 Acc: 0.9970\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9959\n",
      "val Loss: 0.0001 Acc: 0.9970\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9962\n",
      "val Loss: 0.0001 Acc: 0.9980\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9963\n",
      "val Loss: 0.0001 Acc: 0.9975\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9963\n",
      "val Loss: 0.0001 Acc: 0.9975\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9972\n",
      "val Loss: 0.0001 Acc: 0.9980\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9972\n",
      "val Loss: 0.0001 Acc: 0.9980\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9972\n",
      "val Loss: 0.0001 Acc: 0.9975\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9981\n",
      "val Loss: 0.0001 Acc: 0.9980\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9982\n",
      "val Loss: 0.0001 Acc: 0.9975\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9980\n",
      "val Loss: 0.0001 Acc: 0.9980\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 0.9983\n",
      "val Loss: 0.0001 Acc: 0.9980\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 0.9984\n",
      "val Loss: 0.0001 Acc: 0.9980\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 0.9989\n",
      "val Loss: 0.0001 Acc: 0.9970\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 0.9986\n",
      "val Loss: 0.0001 Acc: 0.9980\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 0.9991\n",
      "val Loss: 0.0001 Acc: 0.9985\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 0.9992\n",
      "val Loss: 0.0001 Acc: 0.9980\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 0.9992\n",
      "val Loss: 0.0001 Acc: 0.9980\n",
      "\n",
      "Training complete in 1m 22s\n",
      "Best val Acc: 0.998494\n",
      "initializing param classifier.0.weight\n",
      "initializing param classifier.0.bias\n",
      "initializing param classifier.2.weight\n",
      "initializing param classifier.2.bias\n",
      "initializing param classifier.4.weight\n",
      "initializing param classifier.4.bias\n",
      "********************objective with L2 norm***************\n",
      "dictoinary length1\n",
      "batch number  0\n",
      "batch number  1\n",
      "batch number  2\n",
      "batch number  3\n",
      "batch number  4\n",
      "batch number  5\n",
      "batch number  6\n",
      "batch number  7\n",
      "batch number  8\n",
      "batch number  9\n",
      "batch number  10\n",
      "batch number  11\n",
      "batch number  12\n",
      "batch number  13\n",
      "batch number  14\n",
      "batch number  15\n",
      "batch number  16\n",
      "batch number  17\n",
      "batch number  18\n",
      "batch number  19\n",
      "batch number  20\n",
      "batch number  21\n",
      "batch number  22\n",
      "batch number  23\n",
      "batch number  24\n",
      "batch number  25\n",
      "batch number  26\n",
      "batch number  27\n",
      "batch number  28\n",
      "batch number  29\n",
      "batch number  30\n",
      "batch number  31\n",
      "batch number  32\n",
      "batch number  33\n",
      "batch number  34\n",
      "batch number  35\n",
      "batch number  36\n",
      "batch number  37\n",
      "batch number  38\n",
      "batch number  39\n",
      "batch number  40\n",
      "batch number  41\n",
      "batch number  42\n",
      "batch number  43\n",
      "batch number  44\n",
      "batch number  45\n",
      "batch number  46\n",
      "batch number  47\n",
      "batch number  48\n",
      "batch number  49\n",
      "batch number  50\n",
      "batch number  51\n",
      "batch number  52\n",
      "batch number  53\n",
      "batch number  54\n",
      "batch number  55\n",
      "batch number  56\n",
      "batch number  57\n",
      "batch number  58\n",
      "batch number  59\n",
      "batch number  60\n",
      "batch number  61\n",
      "batch number  62\n",
      "batch number  63\n",
      "storing previous omega classifier.0.weight\n",
      "storing previous omega classifier.0.bias\n",
      "storing previous omega classifier.2.weight\n",
      "storing previous omega classifier.2.bias\n",
      "********************objective with L2 norm***************\n",
      "dictoinary length1\n",
      "batch number  0\n",
      "batch number  1\n",
      "batch number  2\n",
      "batch number  3\n",
      "batch number  4\n",
      "batch number  5\n",
      "batch number  6\n",
      "batch number  7\n",
      "batch number  8\n",
      "batch number  9\n",
      "batch number  10\n",
      "batch number  11\n",
      "batch number  12\n",
      "batch number  13\n",
      "batch number  14\n",
      "batch number  15\n",
      "batch number  16\n",
      "batch number  17\n",
      "batch number  18\n",
      "batch number  19\n",
      "batch number  20\n",
      "batch number  21\n",
      "batch number  22\n",
      "batch number  23\n",
      "batch number  24\n",
      "batch number  25\n",
      "batch number  26\n",
      "batch number  27\n",
      "batch number  28\n",
      "batch number  29\n",
      "batch number  30\n",
      "batch number  31\n",
      "batch number  32\n",
      "batch number  33\n",
      "batch number  34\n",
      "batch number  35\n",
      "batch number  36\n",
      "batch number  37\n",
      "batch number  38\n",
      "batch number  39\n",
      "batch number  40\n",
      "batch number  41\n",
      "batch number  42\n",
      "batch number  43\n",
      "batch number  44\n",
      "batch number  45\n",
      "batch number  46\n",
      "batch number  47\n",
      "batch number  48\n",
      "batch number  49\n",
      "batch number  50\n",
      "batch number  51\n",
      "batch number  52\n",
      "batch number  53\n",
      "batch number  54\n",
      "batch number  55\n",
      "batch number  56\n",
      "batch number  57\n",
      "batch number  58\n",
      "batch number  59\n",
      "restoring previous omega classifier.0.weight\n",
      "restoring previous omega classifier.0.bias\n",
      "restoring previous omega classifier.2.weight\n",
      "restoring previous omega classifier.2.bias\n",
      "classifier.0.weight\n",
      "omega max is tensor(14.8391)\n",
      "omega min is tensor(0.0015)\n",
      "omega mean is tensor(0.6385)\n",
      "classifier.0.bias\n",
      "omega max is tensor(8.0942)\n",
      "omega min is tensor(0.0038)\n",
      "omega mean is tensor(1.1870)\n",
      "classifier.2.weight\n",
      "omega max is tensor(18.6707)\n",
      "omega min is tensor(0.)\n",
      "omega mean is tensor(0.5875)\n",
      "classifier.2.bias\n",
      "omega max is tensor(6.2648)\n",
      "omega min is tensor(2.4152e-05)\n",
      "omega mean is tensor(1.4197)\n",
      "classifier.4.weight\n",
      "classifier.4.bias\n",
      "classifier.0.weight\n",
      "omega max is tensor(14.8391)\n",
      "omega min is tensor(0.0015)\n",
      "omega mean is tensor(0.6385)\n",
      "classifier.0.bias\n",
      "omega max is tensor(8.0942)\n",
      "omega min is tensor(0.0038)\n",
      "omega mean is tensor(1.1870)\n",
      "classifier.2.weight\n",
      "omega max is tensor(18.6707)\n",
      "omega min is tensor(0.)\n",
      "omega mean is tensor(0.5875)\n",
      "classifier.2.bias\n",
      "omega max is tensor(6.2648)\n",
      "omega min is tensor(2.4152e-05)\n",
      "omega mean is tensor(1.4197)\n",
      "classifier.4.weight\n",
      "classifier.4.bias\n",
      "dictoinary length2\n",
      "=> no checkpoint found at 'exp_dir/SGD_MNIST_NET56/epoch.pth.tar'\n",
      "0\n",
      "Epoch 0/19\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0010 Acc: 0.9393\n",
      "val Loss: 0.0006 Acc: 0.9676\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9738\n",
      "val Loss: 0.0006 Acc: 0.9751\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9772\n",
      "val Loss: 0.0006 Acc: 0.9697\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9800\n",
      "val Loss: 0.0005 Acc: 0.9719\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9817\n",
      "val Loss: 0.0004 Acc: 0.9795\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0004 Acc: 0.9829\n",
      "val Loss: 0.0004 Acc: 0.9795\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9839\n",
      "val Loss: 0.0004 Acc: 0.9832\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9854\n",
      "val Loss: 0.0004 Acc: 0.9832\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9860\n",
      "val Loss: 0.0004 Acc: 0.9816\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9866\n",
      "val Loss: 0.0004 Acc: 0.9805\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9847\n",
      "val Loss: 0.0004 Acc: 0.9822\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9847\n",
      "val Loss: 0.0004 Acc: 0.9822\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9875\n",
      "val Loss: 0.0003 Acc: 0.9859\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9889\n",
      "val Loss: 0.0003 Acc: 0.9843\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9884\n",
      "val Loss: 0.0003 Acc: 0.9843\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9878\n",
      "val Loss: 0.0003 Acc: 0.9854\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9888\n",
      "val Loss: 0.0004 Acc: 0.9811\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9895\n",
      "val Loss: 0.0004 Acc: 0.9784\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9864\n",
      "val Loss: 0.0003 Acc: 0.9822\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9894\n",
      "val Loss: 0.0003 Acc: 0.9789\n",
      "\n",
      "Training complete in 1m 18s\n",
      "Best val Acc: 0.985946\n",
      "initializing param classifier.0.weight\n",
      "initializing param classifier.0.bias\n",
      "initializing param classifier.2.weight\n",
      "initializing param classifier.2.bias\n",
      "initializing param classifier.4.weight\n",
      "initializing param classifier.4.bias\n",
      "********************objective with L2 norm***************\n",
      "dictoinary length1\n",
      "batch number  0\n",
      "batch number  1\n",
      "batch number  2\n",
      "batch number  3\n",
      "batch number  4\n",
      "batch number  5\n",
      "batch number  6\n",
      "batch number  7\n",
      "batch number  8\n",
      "batch number  9\n",
      "batch number  10\n",
      "batch number  11\n",
      "batch number  12\n",
      "batch number  13\n",
      "batch number  14\n",
      "batch number  15\n",
      "batch number  16\n",
      "batch number  17\n",
      "batch number  18\n",
      "batch number  19\n",
      "batch number  20\n",
      "batch number  21\n",
      "batch number  22\n",
      "batch number  23\n",
      "batch number  24\n",
      "batch number  25\n",
      "batch number  26\n",
      "batch number  27\n",
      "batch number  28\n",
      "batch number  29\n",
      "batch number  30\n",
      "batch number  31\n",
      "batch number  32\n",
      "batch number  33\n",
      "batch number  34\n",
      "batch number  35\n",
      "batch number  36\n",
      "batch number  37\n",
      "batch number  38\n",
      "batch number  39\n",
      "batch number  40\n",
      "batch number  41\n",
      "batch number  42\n",
      "batch number  43\n",
      "batch number  44\n",
      "batch number  45\n",
      "batch number  46\n",
      "batch number  47\n",
      "batch number  48\n",
      "batch number  49\n",
      "batch number  50\n",
      "batch number  51\n",
      "batch number  52\n",
      "batch number  53\n",
      "batch number  54\n",
      "batch number  55\n",
      "batch number  56\n",
      "batch number  57\n",
      "batch number  58\n",
      "batch number  59\n",
      "batch number  60\n",
      "batch number  61\n",
      "batch number  62\n",
      "batch number  63\n",
      "storing previous omega classifier.0.weight\n",
      "storing previous omega classifier.0.bias\n",
      "storing previous omega classifier.2.weight\n",
      "storing previous omega classifier.2.bias\n",
      "********************objective with L2 norm***************\n",
      "dictoinary length1\n",
      "batch number  0\n",
      "batch number  1\n",
      "batch number  2\n",
      "batch number  3\n",
      "batch number  4\n",
      "batch number  5\n",
      "batch number  6\n",
      "batch number  7\n",
      "batch number  8\n",
      "batch number  9\n",
      "batch number  10\n",
      "batch number  11\n",
      "batch number  12\n",
      "batch number  13\n",
      "batch number  14\n",
      "batch number  15\n",
      "batch number  16\n",
      "batch number  17\n",
      "batch number  18\n",
      "batch number  19\n",
      "batch number  20\n",
      "batch number  21\n",
      "batch number  22\n",
      "batch number  23\n",
      "batch number  24\n",
      "batch number  25\n",
      "batch number  26\n",
      "batch number  27\n",
      "batch number  28\n",
      "batch number  29\n",
      "batch number  30\n",
      "batch number  31\n",
      "batch number  32\n",
      "batch number  33\n",
      "batch number  34\n",
      "batch number  35\n",
      "batch number  36\n",
      "batch number  37\n",
      "batch number  38\n",
      "batch number  39\n",
      "batch number  40\n",
      "batch number  41\n",
      "batch number  42\n",
      "batch number  43\n",
      "batch number  44\n",
      "batch number  45\n",
      "batch number  46\n",
      "batch number  47\n",
      "batch number  48\n",
      "batch number  49\n",
      "batch number  50\n",
      "batch number  51\n",
      "batch number  52\n",
      "batch number  53\n",
      "batch number  54\n",
      "batch number  55\n",
      "batch number  56\n",
      "batch number  57\n",
      "batch number  58\n",
      "batch number  59\n",
      "restoring previous omega classifier.0.weight\n",
      "restoring previous omega classifier.0.bias\n",
      "restoring previous omega classifier.2.weight\n",
      "restoring previous omega classifier.2.bias\n",
      "storing previous omega classifier.0.weight\n",
      "storing previous omega classifier.0.bias\n",
      "storing previous omega classifier.2.weight\n",
      "storing previous omega classifier.2.bias\n",
      "********************objective with L2 norm***************\n",
      "dictoinary length1\n",
      "batch number  0\n",
      "batch number  1\n",
      "batch number  2\n",
      "batch number  3\n",
      "batch number  4\n",
      "batch number  5\n",
      "batch number  6\n",
      "batch number  7\n",
      "batch number  8\n",
      "batch number  9\n",
      "batch number  10\n",
      "batch number  11\n",
      "batch number  12\n",
      "batch number  13\n",
      "batch number  14\n",
      "batch number  15\n",
      "batch number  16\n",
      "batch number  17\n",
      "batch number  18\n",
      "batch number  19\n",
      "batch number  20\n",
      "batch number  21\n",
      "batch number  22\n",
      "batch number  23\n",
      "batch number  24\n",
      "batch number  25\n",
      "batch number  26\n",
      "batch number  27\n",
      "batch number  28\n",
      "batch number  29\n",
      "batch number  30\n",
      "batch number  31\n",
      "batch number  32\n",
      "batch number  33\n",
      "batch number  34\n",
      "batch number  35\n",
      "batch number  36\n",
      "batch number  37\n",
      "batch number  38\n",
      "batch number  39\n",
      "batch number  40\n",
      "batch number  41\n",
      "batch number  42\n",
      "batch number  43\n",
      "batch number  44\n",
      "batch number  45\n",
      "batch number  46\n",
      "batch number  47\n",
      "batch number  48\n",
      "batch number  49\n",
      "batch number  50\n",
      "batch number  51\n",
      "batch number  52\n",
      "batch number  53\n",
      "batch number  54\n",
      "batch number  55\n",
      "batch number  56\n",
      "restoring previous omega classifier.0.weight\n",
      "restoring previous omega classifier.0.bias\n",
      "restoring previous omega classifier.2.weight\n",
      "restoring previous omega classifier.2.bias\n",
      "classifier.0.weight\n",
      "omega max is tensor(20.4560)\n",
      "omega min is tensor(0.0263)\n",
      "omega mean is tensor(0.9447)\n",
      "classifier.0.bias\n",
      "omega max is tensor(11.7755)\n",
      "omega min is tensor(0.1132)\n",
      "omega mean is tensor(1.7384)\n",
      "classifier.2.weight\n",
      "omega max is tensor(22.6326)\n",
      "omega min is tensor(0.)\n",
      "omega mean is tensor(0.9434)\n",
      "classifier.2.bias\n",
      "omega max is tensor(7.1341)\n",
      "omega min is tensor(0.0007)\n",
      "omega mean is tensor(2.2189)\n",
      "classifier.4.weight\n",
      "classifier.4.bias\n",
      "classifier.0.weight\n",
      "omega max is tensor(20.4560)\n",
      "omega min is tensor(0.0263)\n",
      "omega mean is tensor(0.9447)\n",
      "classifier.0.bias\n",
      "omega max is tensor(11.7755)\n",
      "omega min is tensor(0.1132)\n",
      "omega mean is tensor(1.7384)\n",
      "classifier.2.weight\n",
      "omega max is tensor(22.6326)\n",
      "omega min is tensor(0.)\n",
      "omega mean is tensor(0.9434)\n",
      "classifier.2.bias\n",
      "omega max is tensor(7.1341)\n",
      "omega min is tensor(0.0007)\n",
      "omega mean is tensor(2.2189)\n",
      "classifier.4.weight\n",
      "classifier.4.bias\n",
      "dictoinary length2\n",
      "=> no checkpoint found at 'exp_dir/SGD_MNIST_NET78/epoch.pth.tar'\n",
      "0\n",
      "Epoch 0/19\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0011 Acc: 0.9508\n",
      "val Loss: 0.0006 Acc: 0.9770\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9841\n",
      "val Loss: 0.0004 Acc: 0.9805\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9870\n",
      "val Loss: 0.0004 Acc: 0.9840\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9873\n",
      "val Loss: 0.0003 Acc: 0.9860\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9889\n",
      "val Loss: 0.0003 Acc: 0.9870\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9891\n",
      "val Loss: 0.0003 Acc: 0.9850\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9902\n",
      "val Loss: 0.0004 Acc: 0.9800\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9898\n",
      "val Loss: 0.0003 Acc: 0.9820\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9903\n",
      "val Loss: 0.0002 Acc: 0.9880\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9911\n",
      "val Loss: 0.0003 Acc: 0.9870\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9908\n",
      "val Loss: 0.0002 Acc: 0.9885\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9891\n",
      "val Loss: 0.0005 Acc: 0.9765\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9894\n",
      "val Loss: 0.0003 Acc: 0.9870\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9886\n",
      "val Loss: 0.0002 Acc: 0.9870\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9897\n",
      "val Loss: 0.0002 Acc: 0.9930\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9906\n",
      "val Loss: 0.0003 Acc: 0.9880\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9897\n",
      "val Loss: 0.0005 Acc: 0.9755\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9891\n",
      "val Loss: 0.0003 Acc: 0.9870\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9905\n",
      "val Loss: 0.0003 Acc: 0.9870\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9900\n",
      "val Loss: 0.0002 Acc: 0.9900\n",
      "\n",
      "Training complete in 1m 28s\n",
      "Best val Acc: 0.993007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing param classifier.0.weight\n",
      "initializing param classifier.0.bias\n",
      "initializing param classifier.2.weight\n",
      "initializing param classifier.2.bias\n",
      "initializing param classifier.4.weight\n",
      "initializing param classifier.4.bias\n",
      "********************objective with L2 norm***************\n",
      "dictoinary length1\n",
      "batch number  0\n",
      "batch number  1\n",
      "batch number  2\n",
      "batch number  3\n",
      "batch number  4\n",
      "batch number  5\n",
      "batch number  6\n",
      "batch number  7\n",
      "batch number  8\n",
      "batch number  9\n",
      "batch number  10\n",
      "batch number  11\n",
      "batch number  12\n",
      "batch number  13\n",
      "batch number  14\n",
      "batch number  15\n",
      "batch number  16\n",
      "batch number  17\n",
      "batch number  18\n",
      "batch number  19\n",
      "batch number  20\n",
      "batch number  21\n",
      "batch number  22\n",
      "batch number  23\n",
      "batch number  24\n",
      "batch number  25\n",
      "batch number  26\n",
      "batch number  27\n",
      "batch number  28\n",
      "batch number  29\n",
      "batch number  30\n",
      "batch number  31\n",
      "batch number  32\n",
      "batch number  33\n",
      "batch number  34\n",
      "batch number  35\n",
      "batch number  36\n",
      "batch number  37\n",
      "batch number  38\n",
      "batch number  39\n",
      "batch number  40\n",
      "batch number  41\n",
      "batch number  42\n",
      "batch number  43\n",
      "batch number  44\n",
      "batch number  45\n",
      "batch number  46\n",
      "batch number  47\n",
      "batch number  48\n",
      "batch number  49\n",
      "batch number  50\n",
      "batch number  51\n",
      "batch number  52\n",
      "batch number  53\n",
      "batch number  54\n",
      "batch number  55\n",
      "batch number  56\n",
      "batch number  57\n",
      "batch number  58\n",
      "batch number  59\n",
      "batch number  60\n",
      "batch number  61\n",
      "batch number  62\n",
      "batch number  63\n",
      "storing previous omega classifier.0.weight\n",
      "storing previous omega classifier.0.bias\n",
      "storing previous omega classifier.2.weight\n",
      "storing previous omega classifier.2.bias\n",
      "********************objective with L2 norm***************\n",
      "dictoinary length1\n",
      "batch number  0\n",
      "batch number  1\n",
      "batch number  2\n",
      "batch number  3\n",
      "batch number  4\n",
      "batch number  5\n",
      "batch number  6\n",
      "batch number  7\n",
      "batch number  8\n",
      "batch number  9\n",
      "batch number  10\n",
      "batch number  11\n",
      "batch number  12\n",
      "batch number  13\n",
      "batch number  14\n",
      "batch number  15\n",
      "batch number  16\n",
      "batch number  17\n",
      "batch number  18\n",
      "batch number  19\n",
      "batch number  20\n",
      "batch number  21\n",
      "batch number  22\n",
      "batch number  23\n",
      "batch number  24\n",
      "batch number  25\n",
      "batch number  26\n",
      "batch number  27\n",
      "batch number  28\n",
      "batch number  29\n",
      "batch number  30\n",
      "batch number  31\n",
      "batch number  32\n",
      "batch number  33\n",
      "batch number  34\n",
      "batch number  35\n",
      "batch number  36\n",
      "batch number  37\n",
      "batch number  38\n",
      "batch number  39\n",
      "batch number  40\n",
      "batch number  41\n",
      "batch number  42\n",
      "batch number  43\n",
      "batch number  44\n",
      "batch number  45\n",
      "batch number  46\n",
      "batch number  47\n",
      "batch number  48\n",
      "batch number  49\n",
      "batch number  50\n",
      "batch number  51\n",
      "batch number  52\n",
      "batch number  53\n",
      "batch number  54\n",
      "batch number  55\n",
      "batch number  56\n",
      "batch number  57\n",
      "batch number  58\n",
      "batch number  59\n",
      "restoring previous omega classifier.0.weight\n",
      "restoring previous omega classifier.0.bias\n",
      "restoring previous omega classifier.2.weight\n",
      "restoring previous omega classifier.2.bias\n",
      "storing previous omega classifier.0.weight\n",
      "storing previous omega classifier.0.bias\n",
      "storing previous omega classifier.2.weight\n",
      "storing previous omega classifier.2.bias\n",
      "********************objective with L2 norm***************\n",
      "dictoinary length1\n",
      "batch number  0\n",
      "batch number  1\n",
      "batch number  2\n",
      "batch number  3\n",
      "batch number  4\n",
      "batch number  5\n",
      "batch number  6\n",
      "batch number  7\n",
      "batch number  8\n",
      "batch number  9\n",
      "batch number  10\n",
      "batch number  11\n",
      "batch number  12\n",
      "batch number  13\n",
      "batch number  14\n",
      "batch number  15\n",
      "batch number  16\n",
      "batch number  17\n",
      "batch number  18\n",
      "batch number  19\n",
      "batch number  20\n",
      "batch number  21\n",
      "batch number  22\n",
      "batch number  23\n",
      "batch number  24\n",
      "batch number  25\n",
      "batch number  26\n",
      "batch number  27\n",
      "batch number  28\n",
      "batch number  29\n",
      "batch number  30\n",
      "batch number  31\n",
      "batch number  32\n",
      "batch number  33\n",
      "batch number  34\n",
      "batch number  35\n",
      "batch number  36\n",
      "batch number  37\n",
      "batch number  38\n",
      "batch number  39\n",
      "batch number  40\n",
      "batch number  41\n",
      "batch number  42\n",
      "batch number  43\n",
      "batch number  44\n",
      "batch number  45\n",
      "batch number  46\n",
      "batch number  47\n",
      "batch number  48\n",
      "batch number  49\n",
      "batch number  50\n",
      "batch number  51\n",
      "batch number  52\n",
      "batch number  53\n",
      "batch number  54\n",
      "batch number  55\n",
      "batch number  56\n",
      "restoring previous omega classifier.0.weight\n",
      "restoring previous omega classifier.0.bias\n",
      "restoring previous omega classifier.2.weight\n",
      "restoring previous omega classifier.2.bias\n",
      "storing previous omega classifier.0.weight\n",
      "storing previous omega classifier.0.bias\n",
      "storing previous omega classifier.2.weight\n",
      "storing previous omega classifier.2.bias\n",
      "********************objective with L2 norm***************\n",
      "dictoinary length1\n",
      "batch number  0\n",
      "batch number  1\n",
      "batch number  2\n",
      "batch number  3\n",
      "batch number  4\n",
      "batch number  5\n",
      "batch number  6\n",
      "batch number  7\n",
      "batch number  8\n",
      "batch number  9\n",
      "batch number  10\n",
      "batch number  11\n",
      "batch number  12\n",
      "batch number  13\n",
      "batch number  14\n",
      "batch number  15\n",
      "batch number  16\n",
      "batch number  17\n",
      "batch number  18\n",
      "batch number  19\n",
      "batch number  20\n",
      "batch number  21\n",
      "batch number  22\n",
      "batch number  23\n",
      "batch number  24\n",
      "batch number  25\n",
      "batch number  26\n",
      "batch number  27\n",
      "batch number  28\n",
      "batch number  29\n",
      "batch number  30\n",
      "batch number  31\n",
      "batch number  32\n",
      "batch number  33\n",
      "batch number  34\n",
      "batch number  35\n",
      "batch number  36\n",
      "batch number  37\n",
      "batch number  38\n",
      "batch number  39\n",
      "batch number  40\n",
      "batch number  41\n",
      "batch number  42\n",
      "batch number  43\n",
      "batch number  44\n",
      "batch number  45\n",
      "batch number  46\n",
      "batch number  47\n",
      "batch number  48\n",
      "batch number  49\n",
      "batch number  50\n",
      "batch number  51\n",
      "batch number  52\n",
      "batch number  53\n",
      "batch number  54\n",
      "batch number  55\n",
      "batch number  56\n",
      "batch number  57\n",
      "batch number  58\n",
      "batch number  59\n",
      "batch number  60\n",
      "restoring previous omega classifier.0.weight\n",
      "restoring previous omega classifier.0.bias\n",
      "restoring previous omega classifier.2.weight\n",
      "restoring previous omega classifier.2.bias\n",
      "classifier.0.weight\n",
      "omega max is tensor(21.6903)\n",
      "omega min is tensor(0.0289)\n",
      "omega mean is tensor(1.3105)\n",
      "classifier.0.bias\n",
      "omega max is tensor(12.3533)\n",
      "omega min is tensor(0.1051)\n",
      "omega mean is tensor(2.3954)\n",
      "classifier.2.weight\n",
      "omega max is tensor(27.0030)\n",
      "omega min is tensor(0.)\n",
      "omega mean is tensor(1.3525)\n",
      "classifier.2.bias\n",
      "omega max is tensor(9.0344)\n",
      "omega min is tensor(0.0016)\n",
      "omega mean is tensor(3.1921)\n",
      "classifier.4.weight\n",
      "classifier.4.bias\n",
      "classifier.0.weight\n",
      "omega max is tensor(21.6903)\n",
      "omega min is tensor(0.0289)\n",
      "omega mean is tensor(1.3105)\n",
      "classifier.0.bias\n",
      "omega max is tensor(12.3533)\n",
      "omega min is tensor(0.1051)\n",
      "omega mean is tensor(2.3954)\n",
      "classifier.2.weight\n",
      "omega max is tensor(27.0030)\n",
      "omega min is tensor(0.)\n",
      "omega mean is tensor(1.3525)\n",
      "classifier.2.bias\n",
      "omega max is tensor(9.0344)\n",
      "omega min is tensor(0.0016)\n",
      "omega mean is tensor(3.1921)\n",
      "classifier.4.weight\n",
      "classifier.4.bias\n",
      "dictoinary length2\n",
      "=> no checkpoint found at 'exp_dir/SGD_MNIST_NET90/epoch.pth.tar'\n",
      "0\n",
      "Epoch 0/19\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0007 Acc: 0.9690\n",
      "val Loss: 0.0005 Acc: 0.9774\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9849\n",
      "val Loss: 0.0004 Acc: 0.9839\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9863\n",
      "val Loss: 0.0003 Acc: 0.9839\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9864\n",
      "val Loss: 0.0004 Acc: 0.9829\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9873\n",
      "val Loss: 0.0003 Acc: 0.9869\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9876\n",
      "val Loss: 0.0003 Acc: 0.9844\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9886\n",
      "val Loss: 0.0003 Acc: 0.9859\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9883\n",
      "val Loss: 0.0003 Acc: 0.9884\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9880\n",
      "val Loss: 0.0003 Acc: 0.9889\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9896\n",
      "val Loss: 0.0003 Acc: 0.9894\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9891\n",
      "val Loss: 0.0003 Acc: 0.9889\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9890\n",
      "val Loss: 0.0003 Acc: 0.9884\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9892\n",
      "val Loss: 0.0003 Acc: 0.9884\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9901\n",
      "val Loss: 0.0003 Acc: 0.9874\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9901\n",
      "val Loss: 0.0003 Acc: 0.9859\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9911\n",
      "val Loss: 0.0002 Acc: 0.9894\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9907\n",
      "val Loss: 0.0002 Acc: 0.9904\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9907\n",
      "val Loss: 0.0002 Acc: 0.9899\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0002 Acc: 0.9904\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9917\n",
      "val Loss: 0.0002 Acc: 0.9904\n",
      "\n",
      "Training complete in 1m 30s\n",
      "Best val Acc: 0.990447\n"
     ]
    }
   ],
   "source": [
    "#MIMIC the case when samples from the previous tasks are seen in each step      \n",
    "from MAS import *                                                               \n",
    "all_digits=[[3,4],[5,6],[7,8],[9,0]]                                            \n",
    "reg_sets=[]                                                                     \n",
    "dataset_path='data/Pytorch_MNIST_dataset//split12_dataset.pth.tar'              \n",
    "                                                                                \n",
    "exp_dir='exp_dir/SGD_MNIST_NET12'                                               \n",
    "pevious_pathes=[]                                                               \n",
    "reg_lambda=1                                                                    \n",
    "for digits in all_digits:                                                       \n",
    "    reg_sets.append(dataset_path)                                               \n",
    "    model_path=os.path.join(exp_dir,'best_model.pth.tar')                       \n",
    "    pevious_pathes.append(model_path)                                           \n",
    "    dlabel=''                                                                   \n",
    "    for i in digits:                                                            \n",
    "        dlabel=dlabel+str(i)                                                    \n",
    "                                                                                \n",
    "    dataset_path='data/Pytorch_MNIST_dataset//split'+dlabel+'_dataset.pth.tar'  \n",
    "                                                                                \n",
    "    exp_dir='exp_dir/SGD_MNIST_NET'+dlabel                                      \n",
    "                                                                                \n",
    "    num_epochs=20                                                               \n",
    "    data_dirs=None                                                              \n",
    "                                                                                \n",
    "    # now learning a new task with all previous tasks' model/dataset            \n",
    "    MAS_sequence(dataset_path=dataset_path,pevious_pathes=pevious_pathes,previous_task_model_path=model_path,exp_dir=exp_dir,data_dirs=data_dirs,reg_sets=reg_sets,reg_lambda=reg_lambda,batch_size=200,num_epochs=num_epochs,lr=1e-2,norm='L2',b1=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate forgetting                                                            \n",
    "from MAS import *                                                               \n",
    "#estimate forgetting                                                            \n",
    "all_digits=[[1,2],[3,4],[5,6],[7,8],[9,0]]                                      \n",
    "average_forgetting=0                                                            \n",
    "exp_dir='exp_dir/SGD_MNIST_NET12'                                               \n",
    "from Test_sequential  import *                                                  \n",
    "for digits in all_digits:                                                       \n",
    "    dlabel=''                                                                   \n",
    "    for i in digits:                                                            \n",
    "        dlabel=dlabel+str(i)                                                    \n",
    "    if all_digits.index(digits)>0:                                              \n",
    "        exp_dir='exp_dir/SGD_MNIST_NET'+dlabel                                  \n",
    "    previous_model_path=os.path.join(exp_dir,'best_model.pth.tar')  \n",
    "    print(\"previous_model_path:\", previous_model_path)\n",
    "    exp_dir='exp_dir/SGD_MNIST_NET90'                                           \n",
    "    dataset_path='data/Pytorch_MNIST_dataset//split'+dlabel+'_dataset.pth.tar'  \n",
    "    print(\"dataset_path: \",dataset_path )\n",
    "    current_model_path=os.path.join(exp_dir,'best_model.pth.tar')    \n",
    "    print(\"current_model_path:\", current_model_path)\n",
    "    acc2=test_seq_task_performance(previous_model_path=previous_model_path,current_model_path=current_model_path,dataset_path=dataset_path)\n",
    "    print(\"acc2 is:\", acc2)\n",
    "    acc1=test_model(previous_model_path,dataset_path)                           \n",
    "    print(\"acc1 is:\", acc1)\n",
    "    forgetting=acc1-acc2                                                        \n",
    "    average_forgetting=average_forgetting+forgetting     \n",
    "    print(\"\\n\")\n",
    "average_forgetting=float(average_forgetting)/float(len(all_digits))                           \n",
    "print(average_forgetting)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print( average_forgetting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10055304172951232\n"
     ]
    }
   ],
   "source": [
    "print(acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10055304172951232\n"
     ]
    }
   ],
   "source": [
    "print(acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
